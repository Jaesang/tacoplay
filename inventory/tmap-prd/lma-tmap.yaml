---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: helm-toolkit
data:
  chart_name: helm-toolkit
  release: helm-toolkit
  namespace: helm-tookit
  values: {}
  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: helm-toolkit
    reference: release3.5
  dependencies: []
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: mariadb
data:
  chart_name: mariadb
  release: mariadb
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        mariadb: registry.cicd.stg.taco/mariadb:10.2.13
        ingress: registry.cicd.stg.taco/nginx-ingress-controller:0.9.0
        error_pages: registry.cicd.stg.taco/defaultbackend:1.0
        prometheus_create_mysql_user: registry.cicd.stg.taco/mariadb:10.2.13
        prometheus_mysql_exporter: registry.cicd.stg.taco/mysqld-exporter:v0.10.0
        prometheus_mysql_exporter_helm_tests: registry.cicd.stg.taco/heat:pike
        dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
        image_repo_sync: docker.io/docker:17.07.0
      pull_policy: Always
    pod:
      replicas:
        server: 1
    volume:
      enabled: false
      class_name: ceph
    monitoring:
      prometheus:
        enabled: true
        mysqld_exporter:
          scrape: true
  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: mariadb
    reference: release3.5
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ldap
data:
  chart_name: ldap
  release: ldap
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        ldap: registry.cicd.stg.taco/openldap:1.2.0
        bootstrap: registry.cicd.stg.taco/openldap:1.2.0
        dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
        image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
      pull_policy: Always
    storage:
      pvc:
        enabled: false
        size: 1Gi
        class_name: ceph
      host:
        data_path: /data/openstack-helm/ldap
        config_path: /data/openstack-helm/config
    bootstrap:
      enabled: enabled
    data:
      sample: |
        dn: ou=People,dc=cluster,dc=local
        objectclass: organizationalunit
        ou: People
        description: We the People

        # NOTE: Password is "password" without quotes
        dn: uid=taco,ou=People,dc=cluster,dc=local
        objectClass: inetOrgPerson
        objectClass: top
        objectClass: posixAccount
        objectClass: shadowAccount
        objectClass: person
        sn: taco
        cn: taco
        uid: taco
        userPassword: {SSHA}l6OsaU3ABgWFdUAKvnhxohExurSDKIO2
        description: SSHA
        gidNumber: 1000
        uidNumber: 1493
        homeDirectory: /home/taco
        mail: taco@cluster.local
  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: ldap
    reference: release3.5
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: elasticsearch
data:
  chart_name: elasticsearch
  release: elasticsearch
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        apache_proxy: registry.cicd.stg.taco/httpd:2.4
        memory_init: registry.cicd.stg.taco/ubuntu-source-heat-engine:3.0.3
        curator: registry.cicd.stg.taco/curator:5.2.0
        elasticsearch: registry.cicd.stg.taco/elasticsearch-s3:v0.1.0
        ceph_key_placement: registry.cicd.stg.taco/ceph-config-helper:v1.10.3
        s3_bucket: registry.cicd.stg.taco/ceph-config-helper:v1.10.3
        s3_user: registry.cicd.stg.taco/ceph-config-helper:v1.10.3
        helm_tests: registry.cicd.stg.taco/ubuntu-source-heat-engine:3.0.3
        prometheus_elasticsearch_exporter: registry.cicd.stg.taco/elasticsearch_exporter:1.0.1
        dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
        snapshot_repository: registry.cicd.stg.taco/ubuntu-source-heat-engine:3.0.3
        image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
      pull_policy: Always
    pod:
      mounts:
        elasticsearch:
          elasticsearch:
            volumes:
              - name: new-data
                hostPath:
                  path: /tmp/elasticsearch-taco
            volumeMounts:
              - name: new-data
                mountPath: /var/lib/prometheus/data
    conf:
      elasticsearch:
        env:
          java_opts: "-Xms4096m -Xmx4096m"
    monitoring:
      prometheus:
        enabled: true
    storage:
      enabled: false
      pvc:
        name: pvc-elastic
        access_mode: [ "ReadWriteOnce" ]
      requests:
        storage: 2000Gi
      storage_class: ceph
  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: elasticsearch
    reference: release3.5
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: fluent-logging
data:
  chart_name: fluent-logging
  release: fluent-logging
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        fluentbit: registry.cicd.stg.taco/fluent-bit:0.13.6
        fluentd: registry.cicd.stg.taco/ubuntu-source-fluentd:ocata
        prometheus_fluentd_exporter: registry.cicd.stg.taco/fluentd_exporter:v0.1
        dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
        helm_tests: registry.cicd.stg.taco/ubuntu-source-heat-engine:3.0.3
        elasticsearch_template: registry.cicd.stg.taco/ubuntu-source-heat-engine:3.0.3
        image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
      pull_policy: Always
    dependencies:
      static:
        elasticsearch-template:
          services:
            - endpoint: internal
              service: elasticsearch
        fluentbit:
          jobs:
            - elasticsearch-template
          services:
            - endpoint: internal
              service: elasticsearch
    labels:
      fluentbit:
        node_selector_key: fluent-logging
        node_selector_value: enabled
        tolerations: true
    conf:
      fluentbit:
      - service:
          header: service
          Flush: 5
          Daemon: Off
          Log_Level: info
          Parsers_File: parsers.conf
      - containers_tail:
          header: input
          Name: tail
          Tag: kube.*
          Path: /var/log/containers/*.log
          Parser: docker
          DB: /var/log/flb_kube.db
          Mem_Buf_Limit: 5MB
      - kube_filter:
          header: filter
          Name: kubernetes
          Match: kube.*
          Merge_JSON_Log: On
      - syslog_tail:
          header: input
          Name: tail
          Tag: syslog.*
          Path: /var/log/messages
          Parser: syslog-kubelet
          DB: /var/log/flb_kube.db
          Mem_Buf_Limit: 5MB
      - syslog_tail:
          header: input
          Name: tail
          Tag: syslog.*
          Path: /var/log/syslog
          Parser: syslog-kubelet
          DB: /var/log/flb_kube.db
          Mem_Buf_Limit: 5MB
      - cluster_filter:
          header: filter
          Name: record_modifier
          Match: "*"
          record: cluster somewhere-cluster
#          record: tag kube.*
      - elasticsearch_docker:
          header: output
          Name: es
          Match: "kube.*"
          Host: elasticsearch-logging
          Port: 80
          Logstash_Format: On
#          Logstash_Prefix: kube
          HTTP_User: taco
          HTTP_Passwd: password
          Type: fluent
      - elasticsearch_syslog:
          header: output
          Name: es
          Match: "syslog.*"
          Host: elasticsearch-logging
          Port: 80
          Logstash_Format: On
          Logstash_Prefix: syslog
          HTTP_User: taco
          HTTP_Passwd: password
          Type: syslog
      parsers:
      - docker:
          header: parser
          Name: docker
          Format: json
          Time_Key: time
          Time_Format: '%Y-%m-%dT%H:%M:%S.%L'
          Time_Keep: On
      - syslog-kubelet:
          header: parser
          Name: syslog-kubelet
          Format: regex
          Regex: '^(?<time>.*[0-9]{2}:[0-9]{2}:[0-9]{2}) (?<host>[^ ]*) (?<app>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? (?<log>.+)$'
          Time_Key: time
          Time_Format: '%b %e %H:%M:%S'
          Time_Offset: ' +0900'
          Time_Keep: On
      templates:
        fluent:
          template: 'logstash-*'
          index_patterns: 'logstash-*'
          settings:
            number_of_shards: 5
            number_of_replicas: 0
          mappings:
            fluent:
              properties:
                kubernetes:
                  properties:
                    container_name:
                      type: keyword
                      index: true
                    docker_id:
                      type: keyword
                      index: true
                    host:
                      type: keyword
                      index: true
                    labels:
                      properties:
                        app:
                          type: keyword
                          index: true
                        application:
                          type: keyword
                          index: true
                        component:
                          type: keyword
                          index: true
                        release_group:
                          type: keyword
                          index: true
                    namespace_name:
                      type: keyword
                      index: true
                    pod_id:
                      type: keyword
                      index: true
                    pod_name:
                      type: keyword
                      index: true
                log:
                  type: text
        syslog:
          template: 'syslog-*'
          index_patterns: 'syslog-*'
          settings:
            number_of_shards: 5
            number_of_replicas: 0
          mappings:
            syslog:
              properties:
                cluster:
                  type: keyword
                app:
                  type: keyword
                host:
                  type: keyword
                pid:
                  type: integer
                log:
                  type: text
    manifests:
      configmap_bin: true
      configmap_etc: true
      deployment_fluentd: false
      daemonset_fluentbit: true
      job_image_repo_sync: true
      helm_tests: true
      monitoring:
        prometheus:
          configmap_bin: true
          deployment_exporter: true
          service_exporter: true
      secret_elasticsearch: true
      service_fluentd: false
      job_elasticsearch_template: true

  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: fluent-logging
    reference: release3.5
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kibana 
data:
  chart_name: kibana
  release: kibana
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        apache_proxy: registry.cicd.stg.taco/httpd:2.4
        kibana: registry.cicd.stg.taco/kibana:5.6.4
        dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
        image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
      pull_policy: Always
    network:
      kibana:
        ingress:
          public: true
          proxy_body_size: 1024M
        node_port:
          enabled: true
          port: 30001
        port: 5601
  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: kibana
    reference: release3.5
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-process-exporter 
data:
  chart_name: prometheus-process-exporter
  release: prometheus-process-exporter
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        process_exporter: registry.cicd.stg.taco/process-exporter:0.2.11
        dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
        image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
      pull_policy: Always
    labels:
      process_exporter:
        process_selector_key: process-exporter
        process_selector_value: enabled
        tolerations: true
  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: prometheus-process-exporter
    reference: release3.5
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus 
data:
  chart_name: prometheus
  release: prometheus
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        apache_proxy: registry.cicd.stg.taco/httpd:2.4
        prometheus: registry.cicd.stg.taco/prom/prometheus:v2.3.2
        helm_tests: registry.cicd.stg.taco/ubuntu-source-heat-engine:3.0.3
        dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
        image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
      pull_policy: Always
    pod:
      affinity:
        anti:
          type:
            default: requiredDuringSchedulingIgnoredDuringExecution
          topologyKey:
            default: kubernetes.io/hostname
      mounts:
        prometheus:
          prometheus:
            volumes:
              - name: new-data
                hostPath:
                  path: /tmp/prometheus-pv
            volumeMounts:
              - name: new-data
                mountPath: /var/lib/prometheus/data1
      replicas:
        prometheus: 3
    storage:
      enabled: false
      pvc:
        name: prometheus-pvc
        access_mode: [ "ReadWriteOnce" ]
      requests:
        storage: 50Gi
      storage_class: local-sc
    conf:
      prometheus:
        # Consumed by a prometheus helper function to generate the command line flags
        # for configuring the prometheus service
        command_line_flags:
          log.level: info
          query.max_concurrency: 20
          query.timeout: 2m
          storage.tsdb.path: /var/lib/prometheus/data1
          storage.tsdb.retention: 30d
          web.enable_admin_api: true
        scrape_configs:
          global:
            scrape_interval: 60s
            evaluation_interval: 60s
          scrape_configs:
            # NOTE(srwilkers): The job definition for Prometheus should always be
            # listed first, so we can inject the basic auth username and password
            # via the endpoints section
            - job_name: 'prometheus-metrics'
              kubernetes_sd_configs:
              - role: endpoints
              scrape_interval: 60s
              relabel_configs:
              - source_labels:
                  - __meta_kubernetes_service_name
                action: keep
                regex: "prom-metrics"
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scrape
                action: keep
                regex: true
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scheme
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_path
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels:
                  - __address__
                  - __meta_kubernetes_service_annotation_prometheus_io_port
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels:
                  - __meta_kubernetes_namespace
                action: replace
                target_label: kubernetes_namespace
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: instance
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: kubernetes_name
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: job
                replacement: ${1}
            - job_name: kubelet
              scheme: https
              # This TLS & bearer token file config is used to connect to the actual scrape
              # endpoints for cluster components. This is separate to discovery auth
              # configuration because discovery & scraping are two separate concerns in
              # Prometheus. The discovery auth config is automatic if Prometheus runs inside
              # the cluster. Otherwise, more config options have to be provided within the
              # <kubernetes_sd_config>.
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              kubernetes_sd_configs:
              - role: node
              scrape_interval: 45s
              relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)
              - target_label: __address__
                replacement: kubernetes.default.svc:443
              - source_labels:
                  - __meta_kubernetes_node_name
                regex: (.+)
                target_label: __metrics_path__
                replacement: /api/v1/nodes/${1}/proxy/metrics
              - source_labels:
                  - __meta_kubernetes_node_name
                action: replace
                target_label: kubernetes_io_hostname
              # Scrape config for Kubelet cAdvisor.
              #
              # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics
              # (those whose names begin with 'container_') have been removed from the
              # Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to
              # retrieve those metrics.
              #
              # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor
              # HTTP endpoint; use "replacement: /api/v1/nodes/${1}:4194/proxy/metrics"
              # in that case (and ensure cAdvisor's HTTP server hasn't been disabled with
              # the --cadvisor-port=0 Kubelet flag).
              #
              # This job is not necessary and should be removed in Kubernetes 1.6 and
              # earlier versions, or it will cause the metrics to be scraped twice.
            - job_name: 'kubernetes-cadvisor'
    
              # Default to scraping over https. If required, just disable this or change to
              # `http`.
              scheme: https
    
              # This TLS & bearer token file config is used to connect to the actual scrape
              # endpoints for cluster components. This is separate to discovery auth
              # configuration because discovery & scraping are two separate concerns in
              # Prometheus. The discovery auth config is automatic if Prometheus runs inside
              # the cluster. Otherwise, more config options have to be provided within the
              # <kubernetes_sd_config>.
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    
              kubernetes_sd_configs:
              - role: node
    
              relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)
              - target_label: __address__
                replacement: kubernetes.default.svc:443
              - source_labels:
                  - __meta_kubernetes_node_name
                regex: (.+)
                target_label: __metrics_path__
                replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
              # Scrape config for API servers.
              #
              # Kubernetes exposes API servers as endpoints to the default/kubernetes
              # service so this uses `endpoints` role and uses relabelling to only keep
              # the endpoints associated with the default/kubernetes service using the
              # default named port `https`. This works for single API server deployments as
              # well as HA API server deployments.
            - job_name: 'apiserver'
              kubernetes_sd_configs:
              - role: endpoints
              scrape_interval: 45s
              # Default to scraping over https. If required, just disable this or change to
              # `http`.
              scheme: https
              # This TLS & bearer token file config is used to connect to the actual scrape
              # endpoints for cluster components. This is separate to discovery auth
              # configuration because discovery & scraping are two separate concerns in
              # Prometheus. The discovery auth config is automatic if Prometheus runs inside
              # the cluster. Otherwise, more config options have to be provided within the
              # <kubernetes_sd_config>.
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                # If your node certificates are self-signed or use a different CA to the
                # master CA, then disable certificate verification below. Note that
                # certificate verification is an integral part of a secure infrastructure
                # so this should only be disabled in a controlled environment. You can
                # disable certificate verification by uncommenting the line below.
                #
                # insecure_skip_verify: true
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              # Keep only the default/kubernetes service endpoints for the https port. This
              # will add targets for each API server which Kubernetes adds an endpoint to
              # the default/kubernetes service.
              relabel_configs:
              - source_labels:
                  - __meta_kubernetes_namespace
                  - __meta_kubernetes_service_name
                  - __meta_kubernetes_endpoint_port_name
                action: keep
                regex: default;kubernetes;https
            # Scrape config for service endpoints.
            #
            # The relabeling allows the actual service scrape endpoint to be configured
            # via the following annotations:
            #
            # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
            # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
            # to set this to `https` & most likely set the `tls_config` of the scrape config.
            # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
            # * `prometheus.io/port`: If the metrics are exposed on a different port to the
            # service then set this appropriately.
            - job_name: 'openstack-exporter'
              kubernetes_sd_configs:
              - role: endpoints
              scrape_interval: 60s
              relabel_configs:
              - source_labels:
                  - __meta_kubernetes_service_name
                action: keep
                regex: "openstack-metrics"
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scrape
                action: keep
                regex: true
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scheme
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_path
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels:
                  - __address__
                  - __meta_kubernetes_service_annotation_prometheus_io_port
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels:
                  - __meta_kubernetes_namespace
                action: replace
                target_label: kubernetes_namespace
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: instance
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: kubernetes_name
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: job
                replacement: ${1}
            - job_name: 'kubernetes-service-endpoints'
              kubernetes_sd_configs:
              - role: endpoints
              scrape_interval: 60s
              relabel_configs:
              - source_labels:
                  - __meta_kubernetes_service_name
                action: drop
                regex: '(openstack-metrics|prom-metrics)'
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scrape
                action: keep
                regex: true
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scheme
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_path
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels:
                  - __address__
                  - __meta_kubernetes_service_annotation_prometheus_io_port
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels:
                  - __meta_kubernetes_namespace
                action: replace
                target_label: kubernetes_namespace
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: kubernetes_name
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: job
                replacement: ${1}
            # Example scrape config for pods
            #
            # The relabeling allows the actual pod scrape endpoint to be configured via the
            # following annotations:
            #
            # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
            # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
            # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the
            # pod's declared ports (default is a port-free target if none are declared).
            - job_name: 'kubernetes-pods'
              kubernetes_sd_configs:
              - role: pod
              relabel_configs:
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                action: keep
                regex: true
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                action: replace
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
                target_label: __address__
              - action: labelmap
                regex: __meta_kubernetes_pod_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_pod_name]
                action: replace
                target_label: kubernetes_pod_name
            - job_name: calico-etcd
              kubernetes_sd_configs:
              - role: service
              scrape_interval: 20s
              relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - action: keep
                source_labels:
                  - __meta_kubernetes_service_name
                regex: "calico-etcd"
              - action: keep
                source_labels:
                  - __meta_kubernetes_namespace
                regex: kube-system
                target_label: namespace
              - source_labels:
                  - __meta_kubernetes_pod_name
                target_label: pod
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: service
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: job
                replacement: ${1}
              - source_labels:
                  - __meta_kubernetes_service_label
                target_label: job
                regex: calico-etcd
                replacement: ${1}
              - target_label: endpoint
                replacement: "calico-etcd"
  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: prometheus
    reference: release3.5
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-alertmanager 
data:
  chart_name: prometheus-alertmanager
  release: prometheus-alertmanager
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        alertmanager: registry.cicd.stg.taco/alertmanager:v0.11.0
        dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
        image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
      pull_policy: Always
    storage:
      enabled: false
      requests:
        storage: 5Gi
      storage_class: ceph
  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: prometheus-alertmanager
    reference: release3.5
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-kube-state-metrics 
data:
  chart_name: prometheus-kube-state-metrics
  release: prometheus-kube-state-metrics
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        kube_state_metrics: registry.cicd.stg.taco/kube-state-metrics:1.3.1
        dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
        image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
      pull_policy: IfNotPresent
    dummy: Always
  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: prometheus-kube-state-metrics
    reference: release3.5
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-node-exporter 
data:
  chart_name: prometheus-node-exporter
  release: prometheus-node-exporter
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        node_exporter: registry.cicd.stg.taco/node-exporter:v0.15.0
        dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
        image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
      pull_policy: Always
    labels:
      node_exporter:
        node_selector_key: node-exporter
        node_selector_value: enabled
        tolerations: true
  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: prometheus-node-exporter
    reference: release3.5
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-openstack-exporter 
data:
  chart_name: prometheus-openstack-exporter
  release: prometheus-openstack-exporter
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        prometheus_openstack_exporter: registry.cicd.stg.taco/prometheus-openstack-exporter:3231f14419f0c47547ce2551b7d884cd222104e6
        dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
        image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
        ks_user: registry.cicd.stg.taco/ubuntu-source-heat-engine:3.0.3
      pull_policy: Always
    dummy: dummy
  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: prometheus-openstack-exporter
    reference: release3.5
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: grafana 
data:
  chart_name: grafana
  release: grafana
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        grafana: registry.cicd.stg.taco/grafana:5.0.0
        dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
        db_init: registry.cicd.stg.taco/ubuntu-source-heat-engine:3.0.3
        grafana_db_session_sync: registry.cicd.stg.taco/ubuntu-source-heat-engine:3.0.3
        helm_tests: registry.cicd.stg.taco/ubuntu-source-heat-engine:3.0.3
        image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
      pull_policy: Always
    network:
      grafana:
        node_port:
          enabled: true
          port: 30009
  source:
    type: git
    location: https://tde.sktelecom.com/stash/scm/openstack/openstack-helm-infra
    subpath: grafana
    reference: release3.5
  dependencies:
    - helm-toolkit
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: monitoring-infra
data:
  description: "Logging Infrastructure"
  sequenced: False
  chart_group:
    - mariadb
    - prometheus
    - prometheus-alertmanager
    - prometheus-kube-state-metrics
    - prometheus-node-exporter
    - prometheus-openstack-exporter
    - grafana
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: logging-infra
data:
  description: "Logging Infrastructure"
  sequenced: False
  chart_group:
    - ldap
    - elasticsearch
    - fluent-logging
    - kibana
---
schema: armada/Manifest/v1
metadata:
  schema: metadata/Document/v1
  name: lma-manifest
data:
  release_prefix: lma
  chart_groups:
    - logging-infra
    - monitoring-infra
