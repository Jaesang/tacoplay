ntp_service_enabled: false

ceph_origin: repository
ceph_repository: custom
ceph_custom_repo: "http://tacorepo/ceph/ceph.repo"

ceph_stable_release: luminous

monitor_interface: bond0.101
public_network: 192.168.101.0/24
cluster_network: 192.168.102.0/24

osd_objectstore: filestore

ceph_conf_overrides:
  global:
    mon_allow_pool_delete: true
    osd_pool_default_size: 2
    osd_pool_default_min_size: 1
    osd_pg_stat_report_internal_max: 1

    filestore_wbthrottle_enable: false
    filestore_wbthrottle_xfs_bytes_start_flusher: 2147483648
    filestore_wbthrottle_xfs_bytes_hard_limit: 2147484000
    filestore_wbthrottle_xfs_ios_start_flusher: 80000
    filestore_wbthrottle_xfs_ios_hard_limit: 140000
    filestore_wbthrottle_xfs_inodes_start_flusher: 10000

    filestore_queue_max_bytes: 1048576000
    filestore_queue_max_ops: 8000
    filestore_max_sync_interval: 10
    filestore_fd_cache_size: 131072
    filestore_fd_cache_shards: 32
    filestore_op_threads: 20
    filestore_omap_header_cache_size: 5000000
    journal_max_write_entries: 2000
    journal_max_write_bytes: 4194304000

    osd_client_message_size_cap: 0
    osd_client_message_cap: 0
    objecter_inflight_ops: 102400
    objecter_inflight_op_bytes: 1048576000
    ms_dispatch_throttle_bytes: 1048576000

    #osd_op_threads: 32
    osd_op_num_shards: 8
    osd_op_num_threads_per_shard: 2

    mon_pg_warn_max_object_skew: 10000
    mon_pg_warn_min_per_osd: 0
    #mon_pg_warn_max_per_osd: 32768
    #mon_compat_on_trim: false
    mon_max_pool_pg_num: 166496
    mon_osd_max_split_count: 10000
    osd_pg_object_context_cache_count: 512

    leveldb_write_buffer_size: 134217728
    leveldb_cache_size: 536870912
    leveldb_compression: false

openstack_config: true
kube_pool:
  name: "kube"
  pg_num: 1024
  pgp_num: 1024
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
openstack_glance_pool:
  name: "images"
  pg_num: 512
  pgp_num: 512
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
openstack_cinder_pool:
  name: "volumes"
  pg_num: 1024
  pgp_num: 1024
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
openstack_cinder_backup_pool:
  name: "backups"
  pg_num: 64
  pgp_num: 64
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""

openstack_pools:
  - "{{ kube_pool }}"
  - "{{ openstack_glance_pool }}"
  - "{{ openstack_cinder_pool }}"
  - "{{ openstack_cinder_backup_pool }}"

openstack_keys:
        - { name: client.kube, caps: { mon: "profile rbd", osd: "profile rbd pool=kube"}, key: "AQAPn8tUmPBwCxAAeIfvpDKA1fGvrBeXGdc6xQ==", mode: "0600" }
        - { name: client.cinder, caps: { mon: "profile rbd", osd: "profile rbd pool=volumes, profile rbd pool=backups, profile rbd pool=images"}, key: "AQAin8tU0CFgEhAATb7sYgtWsh+S5HEbg6MrGg==",  mode: "0600" }

ceph_mgr_modules: [status,dashboard,prometheus]

osd_scenario: lvm
lvm_volumes:
  - data: /dev/sdb
    journal: /dev/pmem0p1
  - data: /dev/sdc
    journal: /dev/pmem0p2
  - data: /dev/sdd
    journal: /dev/pmem0p3
  - data: /dev/sde
    journal: /dev/pmem0p4
  - data: /dev/sdf
    journal: /dev/pmem0p5
  - data: /dev/sdg
    journal: /dev/pmem0p6
  - data: /dev/sdh
    journal: /dev/pmem0p7
  - data: /dev/sdi
    journal: /dev/pmem0p8
