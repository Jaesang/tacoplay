# use custom repo for real deployment
ceph_stable_release: luminous
ceph_origin: repository
ceph_repository: custom
ceph_custom_repo: "http://ddp-pcl-admin01/ceph/ceph.repo"

ntp_service_enabled: false
copy_admin_key: true
ceph_mgr_modules: [status,dashboard,prometheus]

monitor_interface: bond1.2060@bond1
public_network: 192.168.60.0/24
cluster_network: 192.168.61.0/24

osd_objectstore: bluestore
osd_scenario: lvm
lvm_volumes:
  - data: /dev/sda
  - data: /dev/sdb
  - data: /dev/sdc
  - data: /dev/sdd
  - data: /dev/sde
  - data: /dev/sdf

ceph_conf_overrides:
  global:
    mon_allow_pool_delete: true
    osd_pool_default_size: 3
    osd_pool_default_min_size: 2
    osd_pg_stat_report_internal_max: 1

openstack_config: true
# Total 18 OSDs
#  kube: 3 copy, 10% (Usable 1TB)
#  glance: 3 copy, 10% (Usable 1TB)
#  cinder: 3 copy, 80%
kube_pool:
  name: "kube"
  pg_num: 64
  pgp_num: 64
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
openstack_glance_pool:
  name: "images"
  pg_num: 64
  pgp_num: 64
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
openstack_cinder_pool:
  name: "volumes"
  pg_num: 512
  pgp_num: 512
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""

openstack_pools:
  - "{{ kube_pool }}"
  - "{{ openstack_glance_pool }}"
  - "{{ openstack_cinder_pool }}"

openstack_keys:
  - { name: client.kube, caps: { mon: "profile rbd", osd: "profile rbd pool=kube"}, key: "AQAPn8tUmPBwCxAAeIfvpDKA1fGvrBeXGdc6xQ==", mode: "0600" }
  - { name: client.cinder, caps: { mon: "profile rbd", osd: "profile rbd pool=volumes, profile rbd pool=images"}, key: "AQAin8tU0CFgEhAATb7sYgtWsh+S5HEbg6MrGg==",  mode: "0600" }
