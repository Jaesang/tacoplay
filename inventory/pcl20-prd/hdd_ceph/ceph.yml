# use custom repo for real deployment
ceph_stable_release: luminous
ceph_origin: repository
ceph_repository: custom
ceph_custom_repo: "http://{{ localrepo_yum }}/ceph/ceph.repo"

ntp_service_enabled: false
copy_admin_key: true
ceph_mgr_modules: [status,dashboard,prometheus]

monitor_interface: bond1.2060@bond1
public_network: 192.168.60.0/24
cluster_network: 192.168.61.0/24

osd_objectstore: bluestore
osd_scenario: lvm
lvm_volumes:
  - data: /dev/sdb
  - data: /dev/sdc
  - data: /dev/sdd
  - data: /dev/sde
  - data: /dev/sdf
  - data: /dev/sdg
  - data: /dev/sdh
  - data: /dev/sdi
  - data: /dev/sdj
  - data: /dev/sdk
  - data: /dev/sdl
  - data: /dev/sdm

ceph_conf_overrides:
  global:
    mon_allow_pool_delete: true
    osd_pool_default_size: 3
    osd_pool_default_min_size: 2
    osd_pg_stat_report_internal_max: 1

openstack_config: true
# Total 3.8TB x 48 OSDs
#  cinder: 3 copy, 96%
#  monitoring: 3 copy, usable 2TB, 4%
openstack_cinder_pool:
  name: "volumes"
  pg_num: 2048
  pgp_num: 2048
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
lma_pool:
  name: "lma"
  pg_num: 64
  pgp_num: 64
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""

openstack_pools:
  - "{{ openstack_cinder_pool }}"
  - "{{ lma_pool }}"

openstack_keys:
  - { name: client.kube, caps: { mon: "profile rbd", osd: "profile rbd pool=kube"}, key: "AQAPn8tUmPBwCxAAeIfvpDKA1fGvrBeXGdc6xQ==", mode: "0600" }
  - { name: client.cinder, caps: { mon: "profile rbd", osd: "profile rbd pool=volumes"}, key: "AQAin8tU0CFgEhAATb7sYgtWsh+S5HEbg6MrGg==",  mode: "0600" }
