# use custom repo for real deployment
ceph_stable_release: luminous
ceph_origin: repository
ceph_repository: custom
ceph_custom_repo: "http://ddp-pcl-admin01/ceph/ceph.repo"

ntp_service_enabled: false
copy_admin_key: true
ceph_mgr_modules: [status,dashboard,prometheus]

cluster: ssd_ceph
monitor_interface: bond1.2060
public_network: 192.168.60.0/24
cluster_network: 192.168.61.0/24

osd_objectstore: bluestore
osd_scenario: lvm

ceph_conf_overrides:
  global:
    mon_allow_pool_delete: true
    osd_pool_default_size: 3
    osd_pool_default_min_size: 2
    osd_pg_stat_report_internal_max: 1

crush_rule_config: true

crush_rule_hdd:
  name: hdd-DoNotUse
  root: default
  type: host
  default: false

crush_rule_ssd:
  name: ssd-DoNotUse
  root: default
  type: host
  default: false

crush_rules:
  - "{{ crush_rule_hdd }}"
  - "{{ crush_rule_ssd }}"

create_crush_tree: true

openstack_config: true
# PG Calc: Total 66 OSDs ( 18 SSD + 48 HDD ): all 3 copies
#  lma: 2TB HDD -> 4%
#  cinder-hhd: HDD -> 96%
lma_pool:
  name: "hdd_lma"
  pg_num: 64
  pgp_num: 64
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
openstack_cinder_pool:
  name: "hdd_volumes"
  pg_num: 2048
  pgp_num: 2048
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
openstack_pools:
  - "{{ lma_pool }}"
  - "{{ openstack_cinder_pool }}"

openstack_keys:
  - { name: client.kube, caps: { mon: "profile rbd", osd: "profile rbd pool=hdd_lma"}, key: "AQAPn8tUmPBwCxAAeIfvpDKA1fGvrBeXGdc6xQ==", mode: "0600" }
  - { name: client.cinder, caps: { mon: "profile rbd", osd: "profile rbd pool=hdd_volumes"}, key: "AQAin8tU0CFgEhAATb7sYgtWsh+S5HEbg6MrGg==",  mode: "0600" }
