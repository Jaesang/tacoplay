---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: helm-toolkit
data:
  chart_name: helm-toolkit
  release: helm-toolkit
  namespace: helm-tookit
  values: {}
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: helm-toolkit
  dependencies: []
  test:
    enabled: false
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ingress
data:
  chart_name: ingress
  release: ingress
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    deployment:
      mode: cluster
    network:
      host_namespace: true
    config:
      worker-processes: "8"
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: ingress
  dependencies:
    - helm-toolkit
  test:
    enabled: false
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: mariadb
data:
  chart_name: mariadb
  release: mariadb
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    volume:
      enabled: false
    monitoring:
      prometheus:
        enabled: true
        mysqld_exporter:
          scrape: true
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: mariadb
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ldap
data:
  chart_name: ldap
  release: ldap
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    storage:
      pvc:
        enabled: false
      host:
        data_path: /data/openstack-helm/ldap
        config_path: /data/openstack-helm/config
    bootstrap:
      enabled: enabled
    data:
      sample: |
        dn: ou=People,dc=cluster,dc=local
        objectclass: organizationalunit
        ou: People
        description: We the People

        # NOTE: Password is "password" without quotes
        dn: uid=taco,ou=People,dc=cluster,dc=local
        objectClass: inetOrgPerson
        objectClass: top
        objectClass: posixAccount
        objectClass: shadowAccount
        objectClass: person
        sn: taco
        cn: taco
        uid: taco
        userPassword: {SSHA}l6OsaU3ABgWFdUAKvnhxohExurSDKIO2
        description: SSHA
        gidNumber: 1000
        uidNumber: 1493
        homeDirectory: /home/taco
        mail: taco@cluster.local
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: ldap
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: elasticsearch
data:
  chart_name: elasticsearch
  release: elasticsearch
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    pod:
      mounts:
        elasticsearch:
          elasticsearch:
            volumes:
              - name: new-data
                hostPath:
                  path: /tmp/elasticsearch-taco
            volumeMounts:
              - name: new-data
                mountPath: /var/lib/prometheus/data
    conf:
      elasticsearch:
        env:
          java_opts: "-Xms4096m -Xmx4096m"
    monitoring:
      prometheus:
        enabled: true
    storage:
      enabled: false
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: elasticsearch
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: fluent-logging
data:
  chart_name: fluent-logging
  release: fluent-logging
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    dependencies:
      static:
        elasticsearch-template:
          services:
            - endpoint: internal
              service: elasticsearch
        fluentbit:
          jobs:
            - elasticsearch-template
          services:
            - endpoint: internal
              service: elasticsearch
    labels:
      fluentbit:
        node_selector_key: fluent-logging
        node_selector_value: enabled
        tolerations: true
    conf:
      fluentbit:
      - service:
          header: service
          Flush: 5
          Daemon: Off
          Log_Level: info
          Parsers_File: parsers.conf
      - containers_tail:
          header: input
          Name: tail
          Tag: kube.*
          Path: /var/log/containers/*.log
          Parser: docker
          DB: /var/log/flb_kube.db
          Mem_Buf_Limit: 5MB
      - kube_filter:
          header: filter
          Name: kubernetes
          Match: kube.*
          Merge_JSON_Log: On
      - syslog_tail:
          header: input
          Name: tail
          Tag: syslog.*
          Path: /var/log/messages
          Parser: syslog-kubelet
          DB: /var/log/flb_kube.db
          Mem_Buf_Limit: 5MB
      - syslog_tail:
          header: input
          Name: tail
          Tag: syslog.*
          Path: /var/log/syslog
          Parser: syslog-kubelet
          DB: /var/log/flb_kube.db
          Mem_Buf_Limit: 5MB
      - cluster_filter:
          header: filter
          Name: record_modifier
          Match: "*"
          record: cluster somewhere-cluster
#          record: tag kube.*
      - elasticsearch_docker:
          header: output
          Name: es
          Match: "kube.*"
          Host: elasticsearch-logging
          Port: 80
          Logstash_Format: On
#          Logstash_Prefix: kube
          HTTP_User: taco
          HTTP_Passwd: password
          Type: fluent
      - elasticsearch_syslog:
          header: output
          Name: es
          Match: "syslog.*"
          Host: elasticsearch-logging
          Port: 80
          Logstash_Format: On
          Logstash_Prefix: syslog
          HTTP_User: taco
          HTTP_Passwd: password
          Type: syslog
      parsers:
      - docker:
          header: parser
          Name: docker
          Format: json
          Time_Key: time
          Time_Format: '%Y-%m-%dT%H:%M:%S.%L'
          Time_Keep: On
      - syslog-kubelet:
          header: parser
          Name: syslog-kubelet
          Format: regex
          Regex: '^(?<time>.*[0-9]{2}:[0-9]{2}:[0-9]{2}) (?<host>[^ ]*) (?<app>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? (?<log>.+)$'
          Time_Key: time
          Time_Format: '%b %e %H:%M:%S'
          Time_Offset: ' +0900'
          Time_Keep: On
      templates:
        fluent:
          template: 'logstash-*'
          index_patterns: 'logstash-*'
          settings:
            number_of_shards: 5
            number_of_replicas: 0
          mappings:
            fluent:
              properties:
                kubernetes:
                  properties:
                    container_name:
                      type: keyword
                      index: true
                    docker_id:
                      type: keyword
                      index: true
                    host:
                      type: keyword
                      index: true
                    labels:
                      properties:
                        app:
                          type: keyword
                          index: true
                        application:
                          type: keyword
                          index: true
                        component:
                          type: keyword
                          index: true
                        release_group:
                          type: keyword
                          index: true
                    namespace_name:
                      type: keyword
                      index: true
                    pod_id:
                      type: keyword
                      index: true
                    pod_name:
                      type: keyword
                      index: true
                log:
                  type: text
        syslog:
          template: 'syslog-*'
          index_patterns: 'syslog-*'
          settings:
            number_of_shards: 5
            number_of_replicas: 0
          mappings:
            syslog:
              properties:
                cluster:
                  type: keyword
                app:
                  type: keyword
                host:
                  type: keyword
                pid:
                  type: integer
                log:
                  type: text
    manifests:
      configmap_bin: true
      configmap_etc: true
      deployment_fluentd: false
      daemonset_fluentbit: true
      job_image_repo_sync: true
      helm_tests: false
      monitoring:
        prometheus:
          configmap_bin: true
          deployment_exporter: true
          service_exporter: true
      secret_elasticsearch: true
      service_fluentd: false
      job_elasticsearch_template: true
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: fluent-logging
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kibana 
data:
  chart_name: kibana
  release: kibana
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    network:
      kibana:
        ingress:
          public: true
          proxy_body_size: 1024M
        node_port:
          enabled: true
          port: 30001
        port: 5601
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: kibana
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-process-exporter 
data:
  chart_name: prometheus-process-exporter
  release: prometheus-process-exporter
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    labels:
      process_exporter:
        node_selector_key: process-exporter
        node_selector_value: enabled
        tolerations: true
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: prometheus-process-exporter
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus 
data:
  chart_name: prometheus
  release: prometheus
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    storage:
      enabled: false
    conf:
      prometheus:
        # Consumed by a prometheus helper function to generate the command line flags
        # for configuring the prometheus service
        command_line_flags:
          log.level: info
          query.max_concurrency: 20
          query.timeout: 2m
          storage.tsdb.path: /var/lib/prometheus/data
          storage.tsdb.retention: 30d
          web.enable_admin_api: true
        scrape_configs:
          global:
            scrape_interval: 60s
            evaluation_interval: 60s
          scrape_configs:
            # NOTE(srwilkers): The job definition for Prometheus should always be
            # listed first, so we can inject the basic auth username and password
            # via the endpoints section
            - job_name: 'prometheus-metrics'
              kubernetes_sd_configs:
              - role: endpoints
              scrape_interval: 60s
              relabel_configs:
              - source_labels:
                  - __meta_kubernetes_service_name
                action: keep
                regex: "prom-metrics"
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scrape
                action: keep
                regex: true
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scheme
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_path
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels:
                  - __address__
                  - __meta_kubernetes_service_annotation_prometheus_io_port
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels:
                  - __meta_kubernetes_namespace
                action: replace
                target_label: kubernetes_namespace
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: instance
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: kubernetes_name
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: job
                replacement: ${1}
            - job_name: kubelet
              scheme: https
              # This TLS & bearer token file config is used to connect to the actual scrape
              # endpoints for cluster components. This is separate to discovery auth
              # configuration because discovery & scraping are two separate concerns in
              # Prometheus. The discovery auth config is automatic if Prometheus runs inside
              # the cluster. Otherwise, more config options have to be provided within the
              # <kubernetes_sd_config>.
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              kubernetes_sd_configs:
              - role: node
              scrape_interval: 45s
              relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)
              - target_label: __address__
                replacement: kubernetes.default.svc:443
              - source_labels:
                  - __meta_kubernetes_node_name
                regex: (.+)
                target_label: __metrics_path__
                replacement: /api/v1/nodes/${1}/proxy/metrics
              - source_labels:
                  - __meta_kubernetes_node_name
                action: replace
                target_label: kubernetes_io_hostname
              # Scrape config for Kubelet cAdvisor.
              #
              # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics
              # (those whose names begin with 'container_') have been removed from the
              # Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to
              # retrieve those metrics.
              #
              # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor
              # HTTP endpoint; use "replacement: /api/v1/nodes/${1}:4194/proxy/metrics"
              # in that case (and ensure cAdvisor's HTTP server hasn't been disabled with
              # the --cadvisor-port=0 Kubelet flag).
              #
              # This job is not necessary and should be removed in Kubernetes 1.6 and
              # earlier versions, or it will cause the metrics to be scraped twice.
            - job_name: 'kubernetes-cadvisor'
    
              # Default to scraping over https. If required, just disable this or change to
              # `http`.
              scheme: https
    
              # This TLS & bearer token file config is used to connect to the actual scrape
              # endpoints for cluster components. This is separate to discovery auth
              # configuration because discovery & scraping are two separate concerns in
              # Prometheus. The discovery auth config is automatic if Prometheus runs inside
              # the cluster. Otherwise, more config options have to be provided within the
              # <kubernetes_sd_config>.
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    
              kubernetes_sd_configs:
              - role: node
    
              relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)
              - target_label: __address__
                replacement: kubernetes.default.svc:443
              - source_labels:
                  - __meta_kubernetes_node_name
                regex: (.+)
                target_label: __metrics_path__
                replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
              # Scrape config for API servers.
              #
              # Kubernetes exposes API servers as endpoints to the default/kubernetes
              # service so this uses `endpoints` role and uses relabelling to only keep
              # the endpoints associated with the default/kubernetes service using the
              # default named port `https`. This works for single API server deployments as
              # well as HA API server deployments.
            - job_name: 'apiserver'
              kubernetes_sd_configs:
              - role: endpoints
              scrape_interval: 45s
              # Default to scraping over https. If required, just disable this or change to
              # `http`.
              scheme: https
              # This TLS & bearer token file config is used to connect to the actual scrape
              # endpoints for cluster components. This is separate to discovery auth
              # configuration because discovery & scraping are two separate concerns in
              # Prometheus. The discovery auth config is automatic if Prometheus runs inside
              # the cluster. Otherwise, more config options have to be provided within the
              # <kubernetes_sd_config>.
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                # If your node certificates are self-signed or use a different CA to the
                # master CA, then disable certificate verification below. Note that
                # certificate verification is an integral part of a secure infrastructure
                # so this should only be disabled in a controlled environment. You can
                # disable certificate verification by uncommenting the line below.
                #
                # insecure_skip_verify: true
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              # Keep only the default/kubernetes service endpoints for the https port. This
              # will add targets for each API server which Kubernetes adds an endpoint to
              # the default/kubernetes service.
              relabel_configs:
              - source_labels:
                  - __meta_kubernetes_namespace
                  - __meta_kubernetes_service_name
                  - __meta_kubernetes_endpoint_port_name
                action: keep
                regex: default;kubernetes;https
            # Scrape config for service endpoints.
            #
            # The relabeling allows the actual service scrape endpoint to be configured
            # via the following annotations:
            #
            # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
            # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
            # to set this to `https` & most likely set the `tls_config` of the scrape config.
            # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
            # * `prometheus.io/port`: If the metrics are exposed on a different port to the
            # service then set this appropriately.
            - job_name: 'openstack-exporter'
              kubernetes_sd_configs:
              - role: endpoints
              scrape_interval: 60s
              relabel_configs:
              - source_labels:
                  - __meta_kubernetes_service_name
                action: keep
                regex: "openstack-metrics"
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scrape
                action: keep
                regex: true
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scheme
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_path
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels:
                  - __address__
                  - __meta_kubernetes_service_annotation_prometheus_io_port
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels:
                  - __meta_kubernetes_namespace
                action: replace
                target_label: kubernetes_namespace
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: instance
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: kubernetes_name
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: job
                replacement: ${1}
            - job_name: 'kubernetes-service-endpoints'
              kubernetes_sd_configs:
              - role: endpoints
              scrape_interval: 60s
              relabel_configs:
              - source_labels:
                  - __meta_kubernetes_service_name
                action: drop
                regex: '(openstack-metrics|prom-metrics)'
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scrape
                action: keep
                regex: true
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scheme
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_path
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels:
                  - __address__
                  - __meta_kubernetes_service_annotation_prometheus_io_port
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels:
                  - __meta_kubernetes_namespace
                action: replace
                target_label: kubernetes_namespace
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: kubernetes_name
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: job
                replacement: ${1}
            # Example scrape config for pods
            #
            # The relabeling allows the actual pod scrape endpoint to be configured via the
            # following annotations:
            #
            # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
            # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
            # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the
            # pod's declared ports (default is a port-free target if none are declared).
            - job_name: 'kubernetes-pods'
              kubernetes_sd_configs:
              - role: pod
              relabel_configs:
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                action: keep
                regex: true
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                action: replace
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
                target_label: __address__
              - action: labelmap
                regex: __meta_kubernetes_pod_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_pod_name]
                action: replace
                target_label: kubernetes_pod_name
            - job_name: calico-etcd
              kubernetes_sd_configs:
              - role: service
              scrape_interval: 20s
              relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - action: keep
                source_labels:
                  - __meta_kubernetes_service_name
                regex: "calico-etcd"
              - action: keep
                source_labels:
                  - __meta_kubernetes_namespace
                regex: kube-system
                target_label: namespace
              - source_labels:
                  - __meta_kubernetes_pod_name
                target_label: pod
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: service
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: job
                replacement: ${1}
              - source_labels:
                  - __meta_kubernetes_service_label
                target_label: job
                regex: calico-etcd
                replacement: ${1}
              - target_label: endpoint
                replacement: "calico-etcd"
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: prometheus
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-alertmanager 
data:
  chart_name: prometheus-alertmanager
  release: prometheus-alertmanager
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    storage:
      enabled: false
    pod:
      replicas:
        alertmanager: 3
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: prometheus-alertmanager
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-kube-state-metrics 
data:
  chart_name: prometheus-kube-state-metrics
  release: prometheus-kube-state-metrics
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    dummy: Always
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: prometheus-kube-state-metrics
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-node-exporter 
data:
  chart_name: prometheus-node-exporter
  release: prometheus-node-exporter
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    labels:
      node_exporter:
        node_selector_key: node-exporter
        node_selector_value: enabled
        tolerations: true
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: prometheus-node-exporter
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: grafana 
data:
  chart_name: grafana
  release: grafana
  namespace: lma
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    network:
      grafana:
        node_port:
          enabled: true
          port: 30009
  source:
    type: local
    location: /root/charts/openstack-helm-infra
    subpath: grafana
  dependencies:
    - helm-toolkit
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: monitoring-infra
data:
  description: "Monitoring Infrastructure"
  sequenced: False
  chart_group:
    - ingress
    - mariadb
    - prometheus
    - prometheus-process-exporter
    - prometheus-alertmanager
    - prometheus-kube-state-metrics
    - prometheus-node-exporter
    - grafana
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: logging-infra
data:
  description: "Logging Infrastructure"
  sequenced: False
  chart_group:
    - ldap
    - elasticsearch
    - fluent-logging
    - kibana
---
schema: armada/Manifest/v1
metadata:
  schema: metadata/Document/v1
  name: lma-manifest
data:
  release_prefix: lma
  chart_groups:
    - logging-infra
    - monitoring-infra
