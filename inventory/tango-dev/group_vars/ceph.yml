# use custom repo for real deployment
ceph_stable_release: luminous
ceph_origin: repository
ceph_repository: custom
ceph_custom_repo: "http://tacorepo/ceph/ceph.repo"

ntp_service_enabled: false
copy_admin_key: true
ceph_mgr_modules: [status,dashboard,prometheus]

monitor_interface: bond1
public_network: 90.90.229.0/24
cluster_network: 90.90.229.0/24

osd_objectstore: bluestore
osd_scenario: lvm

ceph_conf_overrides:
  global:
    mon_allow_pool_delete: true
    osd_pool_default_size: 3
    osd_pool_default_min_size: 2
    osd_pg_stat_report_internal_max: 1

openstack_config: true
kube_pool:
  name: "kube"
  pg_num: 256
  pgp_num: 256
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
openstack_glance_pool:
  name: "images"
  pg_num: 256
  pgp_num: 256
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""

openstack_pools:
  - "{{ kube_pool }}"
  - "{{ openstack_glance_pool }}"

openstack_keys:
  - { name: client.kube, caps: { mon: "profile rbd", osd: "profile rbd pool=kube"}, key: "AQAPn8tUmPBwCxAAeIfvpDKA1fGvrBeXGdc6xQ==", mode: "0600" }
  - { name: client.cinder, caps: { mon: "profile rbd", osd: "profile rbd pool=volumes, profile rbd pool=backups, profile rbd pool=images"}, key: "AQAin8tU0CFgEhAATb7sYgtWsh+S5HEbg6MrGg==",  mode: "0600" }
